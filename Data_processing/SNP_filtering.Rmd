---
title: "SNP filtering using vcfR"
author: "D. G. Gannon"
date: "September 2020"
output:
  github_document:
    pandoc_args: --webtex
bibliography: /Users/dusty/Documents/zotero_library.bib
header-includes:
  \usepackage{setspace}
  \onehalfspacing
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

  require(vcfR)
  require(stringr)
  require(dplyr)

```

  Previous quality filters up to this point include:  
  
  1) Demultiplexing phase: Removing reads for which Phred score falls below 10 in a 15% sliding window (\texttt{process\_radtags} program)
  
  2) Demultiplexing phase: Removing reads with an uncalled base (\texttt{process\_radtags} program)
  
  3) Catalog phase: Discard alignments with Phred scores < 20 (i.e. only keep alignments for which $\text{Pr}(\text{Incorrect alignment} < 0.01)$; \texttt{gstacks} program)
  
  4) Genotyping: Loci must be present in at least 80% of individuals sampled and have a minor allele frequency of at least 0.05
  
  5) Locus filters: vcf file is filtered to include only bi-allelic loci using \texttt{vcftools}.
  
  

```{bash, eval=FALSE}
  vcftools --vcf ./AQFO_snps_R80.vcf --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all \
  --out ./AQFO_snps_R80_bi
```


### Filtering SNPs based on depth

  Repetitive or similar regions in the genome may result in reads that come from different locations in the genome getting erroneously merged and aligned. This can result in exceptionally deep coverage on certain loci and could result in spurious SNP calls. We set a depth threshold for snps as the $99^\text{th}$ percentile of the empirical distribution of mean SNP depth across samples. This is based on the quantile plot in Figure 1 where one can see that a few SNPs have extremely high coverage.
  
  We do not remove SNPs with low depth because the model that calls genotypes in \texttt{Stacks v2} avoids calling genotypes when uncertainty in the call is high [@maruki2017]. 
  
```{r, message=FALSE, include=FALSE}

  col.vcf <- read.vcfR("~/Documents/Columbine/Data/GBS_Data/AQFO_snps_R80_bi.vcf")

  coverage <- read.table("~/Documents/Columbine/Data/GBS_Data/coverage_per_sample.txt", header = T, 
                         sep = "\t", as.is = T)

```


```{r mean depth}

  dp <- extract.gt(col.vcf, element = "DP", as.numeric = T)

  # heatmap.bp(dp, rlabels = F, clabels = F)
  
  mean_dp_snps <- apply(dp, 1, mean, na.rm=T)
  
  plot(log(quantile(mean_dp_snps, probs=c(seq(0,1, by=0.01)))),
       xlab="quantile", ylab="log(depth)")
```

**Figure 1**: Empirical CDF of the mean depth across samples for each SNP.

```{r }
# Remove snps that have extremely high average read depths
 # what value represents the 98% quantile?
  quant <- quantile(mean_dp_snps, probs=0.99)

# list the snps which have an average depth above the 98% quantile
  snps2_rmv <- which(mean_dp_snps > quant)
  
# we also want to remove snps without any neighbors
#  for the sake of imputation
  snps_per_chrom <- as.data.frame(col.vcf@fix) %>% group_by(., CHROM) %>%
    summarise(n=n())
  
  scaffs2_rmv <- snps_per_chrom$CHROM[which(snps_per_chrom$n == 1)]
  snps2_rmv <- c(snps2_rmv, 
                 which(col.vcf@fix[,1] %in% scaffs2_rmv))
  
  site2rm <- col.vcf@fix[snps2_rmv, 3]

  # write.table(site2rm, "~/Documents/Columbine/Data/GBS_Data/exclude_sites.txt",
  #             quote = F, sep = "\t",
  #           row.names = F, col.names = F)
  
```


$~$

Using the list of SNPs to exclude, we use vcftools to rewrite the vcf file with only the loci we wish to keep. We do not use `vcfR` to do this, as BEAGLE does not recognize files written using `vcfR`.

$~$
 
 
```{bash, eval=FALSE}
  vcftools --vcf ./AQFO_snps_R80_bi.vcf --exclude ./exclude_sites.txt --recode --recode-INFO-all \
  --out ./AQFO_snps_postfilt
```


$~$

```{r, include=FALSE}
  
  col.vcf.good <- read.vcfR("~/Documents/Columbine/Data/GBS_Data/AQFO_snps_postfilt.vcf")
  col_gt <- extract.gt(col.vcf.good, element = "GT")
  
```



 With this vcf file containing `r round(sum(is.na(col_gt))/(nrow(col_gt)*ncol(col_gt)),4)*100` percent missing information, `r dim(col_gt)[1]` SNPs and 192 individual plants, we impute the missing genotypes using BEAGLE [@browning2016].
 
 
```{bash eval=FALSE}
# running BEAGLE through the terminal
  java -Xmx8g -jar ./beagle.25Nov19.28d.jar gt=AQFO_snps_postfilt.vcf out=AQFO_snps_impute nthreads=2
```
$~$

### Filtering SNPs based on exon annotations

Read in the annotations file from the *A. coerulea* genome and store as a dataframe.

```{r annotations}

# read in the imputed vcf
  col_vcf_gdimp <- read.vcfR(file = "~/Documents/Columbine/Data/GBS_Data/AQFO_snps_impute.vcf")
  ann <- read.table("~/Documents/Columbine/Data/GBS_Data/Acoerulea_322_v3.1.gene_exons.gff3",
                    header = F, sep = "\t", as.is = T)[,c(1,3:5,7)]

# name columns
  names(ann) <- c("Chrom", "annot", "start", "stop", "strand")
  
# structure of the data
  head(ann)
  
```

```{r annotate, eval=FALSE}
# annotate snps
  snps <- as.data.frame(col_vcf_gdimp@fix)
  snps$POS <- as.numeric(snps$POS)
  snps$strand <- str_split(snps$ID, ":", simplify = T)[,3]
  snps$SOFA <- NA
  
  for(i in 1:nrow(snps)){
    annots_i <- which((ann$Chrom == snps$CHROM[i]) &
           (ann$start < snps$POS[i]) &
            (ann$stop > snps$POS[i]))
    if(length(annots_i) == 1){
      snps$SOFA[i] <- ann$annot[annots_i]
    } else if(length(annots_i) > 1){
      snps$SOFA[i] <- paste(sort(unique(ann[annots_i,]$annot)), collapse = ":")
    } else {
      snps$SOFA[i] <- "NC"
    }
  }
  
# create "neutral" vcf
  
  col_vcf_nc <- col_vcf_gdimp[which(snps$SOFA == "NC"), ]
  col_vcf_cd <- col_vcf_gdimp[which(snps$SOFA != "NC"), ]
  
  # write.vcf(col_vcf_nc, file = "~/Documents/Columbine/Data/GBS_Data/AQFO_snps_nc.vcf")
  # write.vcf(col_vcf_cd, file = "~/Documents/Columbine/Data/GBS_Data/AQFO_snps_cd.vcf")
  
```


 
$~$

\subsubsection{References}









  
  
  